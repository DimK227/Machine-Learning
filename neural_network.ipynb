{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9_BL-0KIv53p"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "import math\n",
        "import torch.nn.functional as nnf\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from itertools import cycle\n",
        "from sklearn.preprocessing import label_binarize\n",
        "import torch\n",
        "torch.manual_seed(0)\n",
        "import random\n",
        "random.seed(0)\n",
        "np.random.seed(0)\n",
        "\n",
        "def load_glove_model(File):                       #Load glove\n",
        "    print(\"Loading Glove Model\")\n",
        "    glove_model = {}\n",
        "    with open(File,'r') as f:\n",
        "        for line in f:\n",
        "            split_line = line.split()\n",
        "            word = split_line[0]\n",
        "            embedding = np.array(split_line[1:], dtype=np.float64)\n",
        "            glove_model[word] = embedding\n",
        "    print(f\"{len(glove_model)} words loaded!\")\n",
        "    return glove_model\n",
        "\n",
        "nltk.download('punkt')\n",
        "data = pd.read_csv(\"./vaccine_train_set.csv\")     #Take the training set\n",
        "X1 = data.drop('label', axis=1)\n",
        "df_x = X1['tweet']    #keep the tweet\n",
        "Y = data['label'] #Only keep value\n",
        "\n",
        "\n",
        "df_val = pd.read_csv(\"./vaccine_validation_set.csv\")    #Here just change the name of the file\n",
        "X1_val = df_val.drop('label', axis=1)\n",
        "df_x_val = X1_val['tweet']  #Keep the tweet\n",
        "Y_val = df_val['label'] #Only keep value\n",
        "\n",
        "\n",
        "!wget http://nlp.stanford.edu/data/glove.42B.300d.zip   #Download the corpus\n",
        "!unzip glove.42B.300d.zip\n",
        "\n",
        "num_embeddings = 300\n",
        "model1 = load_glove_model(\"./glove.42B.300d.txt\")   #Creat glove model\n",
        "\n",
        "X = []\n",
        "for tweet in df_x:      #For each tweet in dataset\n",
        "  array = []\n",
        "  tokens = nltk.word_tokenize(tweet)\n",
        "  sentence = []\n",
        "  num_words = len(tokens)\n",
        "  for word in tokens:   #For each word in the tweet\n",
        "    \n",
        "    if word in model1:    #If it exists\n",
        "      sentence.append(model1[word])\n",
        "    else:                 #If it doesn't create an empty vector\n",
        "      unknown = [0] * num_embeddings \n",
        "      sentence.append(unknown)\n",
        "  new_sentence = []\n",
        "  for embedding in range(num_embeddings):     #Find the average to have the same size in all tweets\n",
        "    sum = 0\n",
        "    for word in range(num_words):\n",
        "      sum = sum + sentence[word][embedding]\n",
        "    avg = sum/num_embeddings\n",
        "    new_sentence.append(avg)\n",
        "  X.append(new_sentence)\n",
        "\n",
        "#Do the same for the validation file\n",
        "X_val = []\n",
        "for tweet in df_x_val:\n",
        "  array = []\n",
        "  tokens = nltk.word_tokenize(tweet)\n",
        "  sentence = []\n",
        "  num_words = len(tokens)\n",
        "  for word in tokens:\n",
        "    \n",
        "    if word in model1:\n",
        "      sentence.append(model1[word])\n",
        "    else:\n",
        "      unknown = [0] * num_embeddings\n",
        "      sentence.append(unknown)\n",
        "  new_sentence = []\n",
        "  for embedding in range(num_embeddings):\n",
        "    sum = 0\n",
        "    for word in range(num_words):\n",
        "      sum = sum + sentence[word][embedding]\n",
        "    avg = sum/num_embeddings\n",
        "    new_sentence.append(avg)\n",
        "  X_val.append(new_sentence)\n",
        "print(X_val)\n",
        "\n",
        "class Net(nn.Module):     #Neural network\n",
        "    def __init__(self, D_in, H1, H2, H3, D_out):\n",
        "        super(Net, self).__init__()\n",
        "        \n",
        "        self.linear1 = nn.Linear(D_in, H1)\n",
        "        self.linear2 = nn.Linear(H1, H2)\n",
        "        self.linear3 = nn.Linear(H2, H3)\n",
        "        self.linear4 = nn.Linear(H3, D_out)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.logsimoid = nn.LogSigmoid()\n",
        "        self.rrelu =nn.RReLU()\n",
        "        self.logsoftmax = nn.LogSoftmax()\n",
        "        \n",
        "        \n",
        "    def forward(self, x):\n",
        "        h1 = self.linear1(x)\n",
        "        #h1 = self.logsoftmax(h1)\n",
        "        h2 = self.linear2(h1)\n",
        "        h2 = self.relu(h2)\n",
        "        h3 = self.linear3(h2)\n",
        "        #h3 = self.rrelu(h3)\n",
        "        out = self.linear4(h3)\n",
        "        #out = self.rrelu(out)\n",
        "        return out\n",
        "        #return nnf.softmax(out)\n",
        "\n",
        "#Save in tensors\n",
        "x = torch.tensor(X, dtype=torch.float)\n",
        "y = torch.tensor(Y, dtype=torch.float)\n",
        "\n",
        "x_val = torch.tensor(X_val, dtype=torch.float)\n",
        "y_val = torch.tensor(Y_val, dtype=torch.float)\n",
        "\n",
        "y = y.type(torch.LongTensor)\n",
        "y_val = y_val.type(torch.LongTensor)\n",
        "\n",
        "torch.manual_seed(0)\n",
        "random.seed(0)\n",
        "np.random.seed(0)\n",
        "\n",
        "#Create the neiral network\n",
        "D_in = x.shape[1]\n",
        "H1 = 128\n",
        "H2 = 64\n",
        "H3 = 32\n",
        "#H4 = 16\n",
        "D_out = 3\n",
        "\n",
        "#Define Hyperparameters\n",
        "#learning_rate = 1e-1\n",
        "learning_rate = 1e-2\n",
        "\n",
        "model = Net(D_in, H1, H2, H3,D_out)\n",
        "#model = Net(D_in, H1, H2, H3, H4, D_out)\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate) #optimizer\n",
        "\n",
        "loss_func = nn.CrossEntropyLoss()   #Loss func\n",
        "dataset = torch.utils.data.TensorDataset(x, y)\n",
        "dataloader = torch.utils.data.DataLoader(dataset, batch_size=64)\n",
        "#dataloader = torch.utils.data.DataLoader(dataset, batch_size=128)\n",
        "\n",
        "dataset_val = torch.utils.data.TensorDataset(x_val, y_val)\n",
        "#dataloader_val = torch.utils.data.DataLoader(dataset_val, batch_size=64, shuffle=True)\n",
        "dataloader_val = torch.utils.data.DataLoader(dataset_val, batch_size=512)\n",
        "\n",
        "losses=[]\n",
        "losses2=[]\n",
        "epoches = []\n",
        "for epoch in range(100):\n",
        "  epoches.append(epoch)\n",
        "  batch_losses = []\n",
        "  sum = 0\n",
        "  sum1 = 0\n",
        "  for x_batch, y_batch in dataloader:\n",
        "    y_pred = model(x_batch)\n",
        "    y_pred2 = []\n",
        "    loss = loss_func(y_pred, y_batch)\n",
        "    sum = sum + loss.item()\n",
        "    batch_losses.append(loss.item())\n",
        "    #Delete previously stored gradients\n",
        "    optimizer.zero_grad()\n",
        "    #Perform backpropagation starting from the loss calculated in this epoch\n",
        "    loss.backward()\n",
        "    #Update model's weights based on the gradients calculated during backprop\n",
        "    optimizer.step()\n",
        "  x1 = torch.tensor(X_val, dtype=torch.float)\n",
        "  Y_predict = model(x1)\n",
        "  loss = loss_func(Y_predict, y_val)\n",
        "  sum1 = sum1 + loss.item()\n",
        "  batch_losses.append(loss.item()) \n",
        "  Y_predict = nnf.softmax(Y_predict, dim=1)\n",
        "  y_pred2 = []\n",
        "  a = sum/len(dataloader)   #training loss\n",
        "  losses.append(a)\n",
        "  a = sum1/len(dataloader_val) #validation loss\n",
        "  losses2.append(a)\n",
        "#plot loss vs epoch\n",
        "plt.plot(epoches, losses, 'r')\n",
        "plt.plot(epoches, losses, 'g')\n",
        "plt.title(\"TRAINING LOSS\")\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.show()\n",
        "\n",
        "plt.plot(epoches, losses2, 'r')\n",
        "plt.plot(epoches, losses2, 'g')\n",
        "plt.title(\"VALIDATION LOSS\")\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.show()\n",
        "\n",
        "#find f1 score\n",
        "x = torch.tensor(X_val, dtype=torch.float)\n",
        "Y_predict = model(x) \n",
        "Y_predict = nnf.softmax(Y_predict, dim=1)\n",
        "y_pred2 = []\n",
        "for tweet in Y_predict:\n",
        "    if max(tweet) == tweet[0]:\n",
        "       y_pred2.append(0)\n",
        "    elif max(tweet) == tweet[1]:\n",
        "       y_pred2.append(1)\n",
        "    elif max(tweet) == tweet[2]:\n",
        "       y_pred2.append(2)\n",
        "\n",
        "precision = precision_score(Y_val, y_pred2, average='weighted')\n",
        "recall = recall_score(Y_val, y_pred2, average='weighted')\n",
        "f1 = 2*(precision * recall) / (precision + recall)\n",
        "print(f1)\n",
        "#Plot ROC curve\n",
        "y = label_binarize(Y_val, classes=[0, 1, 2])\n",
        "n_classes = 3\n",
        "lw=2\n",
        "fpr = dict()\n",
        "tpr = dict()\n",
        "roc_auc = dict()\n",
        "Y_predict = Y_predict.detach().numpy()\n",
        "for i in range(n_classes):\n",
        "    fpr[i], tpr[i], _ = roc_curve(y[:, i], Y_predict[:, i])\n",
        "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "colors = cycle(['blue', 'red', 'green'])\n",
        "for i, color in zip(range(n_classes), colors):\n",
        "    plt.plot(fpr[i], tpr[i], color=color, lw=2,\n",
        "             label='ROC curve of class {0} (area = {1:0.2f})'\n",
        "             ''.format(i, roc_auc[i]))\n",
        "plt.plot([0, 1], [0, 1], 'k--', lw=lw)\n",
        "plt.xlim([-0.05, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver operating characteristic for multi-class data')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()"
      ]
    }
  ]
}